seed: 42
do_train: True
do_predict: True

# top-level config #
paradigm: mrc
task_name: EAE
dataset_name: ACE2005-en
language: English
test_exists_labels: True

# file path #
output_dir: output/ACE2005-en
type2id_path: ../../data/processed/ace2005-en/label2id.json
role2id_path: ../../data/processed/ace2005-en/role2id.json
train_file: ../../data/processed/ace2005-en/train.unified.jsonl
validation_file: ../../data/processed/ace2005-en/valid.unified.jsonl
test_file: ../../data/processed/ace2005-en/test.unified.jsonl
# event detection predictions
golden_trigger: False
# train_pred_file: output/ACE2005-en/ED/token_classification/bert-base-uncased-marker/train_preds.json
validation_pred_file: ../ED/output/ACE2005-en/ED/mrc/bert-base-uncased-none/valid_preds.json
test_pred_file: ../ED/output/ACE2005-en/ED/mrc/bert-base-uncased-none/test_preds.json
# config for some specific paradigms
prompt_file: ../../config/all-models/eae/mrc/description_queries_eeqa_dygie.csv # used for MRC paradigm
mrc_template_id: 3

# config for data processor # 
truncate_in_batch: False 
return_token_type_ids: False

# model config #
model_type: bert
model_name_or_path: bert-base-uncased
hidden_size: 768
aggregation: none
head_type: mrc

# training config #
num_train_epochs: 10
max_seq_length: 160
dataloader_num_workers: 2

per_device_train_batch_size: 64
per_device_eval_batch_size: 64
gradient_accumulation_steps: 1
# eval_accumulation_steps: 4
learning_rate: 5.0e-5
weight_decay: 1.0e-5
warmup_ratio: 0.1 
max_grad_norm: 1
optim: adamw_torch
early_stopping_patience: 100

load_best_model_at_end: True
metric_for_best_model: micro_f1 
greater_is_better: True 

logging_strategy: steps
logging_steps: 100
evaluation_strategy: epoch
eval_steps: 500
save_strategy: epoch
save_steps: 500

# evaluate/test config #
eae_eval_mode: strict

# split inference #
split_infer: False
split_infer_size: 5000

